{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5840ae2b-9c67-4c2b-867f-bac5f73a0289",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fa4b199-011e-434a-96a9-cea6c80f1d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSample data = \"Hi, this is text analysis practical ! Hope you will enjoy it.\"\\n1. Cleaning = ,.!?#$&\"\"\\'\\'[]{}()\\n2. Tokenization = [\\'Hi\\', \\'this\\']\\n3. POS tagging = It return the part of speech\\n4. Stop word removal = stop words like is, the , a, etc\\n5. Stemming = playing => play, articles => article , stremming based on some rules, analysis\\n6. Lemmatization = search in dictionary and get actual word mice => mouse, playing => play\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample data = \"Hi, this is text analysis practical ! Hope you will enjoy it.\"\n",
    "1. Cleaning = ,.!?#$&\"\"''[]{}()\n",
    "2. Tokenization = ['Hi', 'this']\n",
    "3. POS tagging = It return the part of speech\n",
    "4. Stop word removal = stop words like is, the , a, etc\n",
    "5. Stemming = playing => play, articles => article , stremming based on some rules, analysis\n",
    "6. Lemmatization = search in dictionary and get actual word mice => mouse, playing => play\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7978221-3b3a-4416-b247-40e88693234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akhilesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akhilesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akhilesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Akhilesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f8ef7-0ab8-4aee-ae2f-f9d4711ea2f9",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e712427-b9bf-4276-9804-74e7c9a78158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi this is text analysis practical  hoping you enjoyed it do you liked it playing gaming mice words running boring better went'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = \"Hi, this is text analysis practical ! Hoping you enjoyed it. Do you liked it? playing, gaming, mice, words, running, boring, better, went\"\n",
    "\n",
    "def clean_data(data):\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', data)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "cleaned_data = clean_data(sample_data)\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9696ab-b170-4735-b949-26de39e6a25f",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd84f03c-e361-4a96-b16d-3b505abe7dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'this',\n",
       " 'is',\n",
       " 'text',\n",
       " 'analysis',\n",
       " 'practical',\n",
       " 'hoping',\n",
       " 'you',\n",
       " 'enjoyed',\n",
       " 'it',\n",
       " 'do',\n",
       " 'you',\n",
       " 'liked',\n",
       " 'it',\n",
       " 'playing',\n",
       " 'gaming',\n",
       " 'mice',\n",
       " 'words',\n",
       " 'running',\n",
       " 'boring',\n",
       " 'better',\n",
       " 'went']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(cleaned_data)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ca765-8f81-466c-8d08-5148fcc926b2",
   "metadata": {},
   "source": [
    "### POS Tagging (parts of speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "119bb5d3-1fef-4b44-8288-164d43730b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('text', 'JJ'),\n",
       " ('analysis', 'NN'),\n",
       " ('practical', 'JJ'),\n",
       " ('hoping', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('enjoyed', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('do', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('liked', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('playing', 'VBG'),\n",
       " ('gaming', 'VBG'),\n",
       " ('mice', 'JJ'),\n",
       " ('words', 'NNS'),\n",
       " ('running', 'VBG'),\n",
       " ('boring', 'NN'),\n",
       " ('better', 'RBR'),\n",
       " ('went', 'VBD')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ed5be-d844-4309-90e4-fb2904617703",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf226c23-3d39-4c65-95f6-64cf5afaa88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'text',\n",
       " 'analysis',\n",
       " 'practical',\n",
       " 'hoping',\n",
       " 'enjoyed',\n",
       " 'liked',\n",
       " 'playing',\n",
       " 'gaming',\n",
       " 'mice',\n",
       " 'words',\n",
       " 'running',\n",
       " 'boring',\n",
       " 'better',\n",
       " 'went']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\"\"\"\n",
    "Method 1 : \n",
    "def remove_stwd(tokens, stopwords, list):\n",
    "    for word in tokens:\n",
    "        if word not in stopwords:\n",
    "            list.append(word)\n",
    "    return list\n",
    "print(remove_stwd(tokens, stop_words, []))\n",
    "\"\"\"\n",
    "\n",
    "#Method 2\n",
    "\n",
    "rem_stwords = [word for word in tokens if word not in stop_words]\n",
    "rem_stwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb59571-0212-4c65-ac53-b2c233bfb0a4",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3665d867-a73b-4fca-9a45-cf957d7b2549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'text',\n",
       " 'analysi',\n",
       " 'practic',\n",
       " 'hope',\n",
       " 'enjoy',\n",
       " 'like',\n",
       " 'play',\n",
       " 'game',\n",
       " 'mice',\n",
       " 'word',\n",
       " 'run',\n",
       " 'bore',\n",
       " 'better',\n",
       " 'went']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_word = [stemmer.stem(word) for word in rem_stwords]\n",
    "stemmed_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b754f-9405-4191-bc0b-5d98983afa30",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ef63151-7d1f-4c84-a620-95b84113460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'text',\n",
       " 'analysis',\n",
       " 'practical',\n",
       " 'hoping',\n",
       " 'enjoyed',\n",
       " 'liked',\n",
       " 'playing',\n",
       " 'gaming',\n",
       " 'mouse',\n",
       " 'word',\n",
       " 'running',\n",
       " 'boring',\n",
       " 'better',\n",
       " 'went']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in rem_stwords]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f8611-61da-408a-94af-8c555c5dd222",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "1. TF = no. of t terms in doc/ total number of terms in doc\n",
    "2. IDF = log(no. of documents in collection/ no. of documents containing term t)\n",
    "3. TF-IDF Score = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c29acc0-31c0-4657-b56a-31ea8bdcf2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t0.19611613513818404\n",
      "  (0, 1)\t0.19611613513818404\n",
      "  (0, 2)\t0.19611613513818404\n",
      "  (0, 14)\t0.19611613513818404\n",
      "  (0, 18)\t0.19611613513818404\n",
      "  (0, 11)\t0.19611613513818404\n",
      "  (0, 5)\t0.19611613513818404\n",
      "  (0, 12)\t0.19611613513818404\n",
      "  (0, 10)\t0.19611613513818404\n",
      "  (0, 3)\t0.19611613513818404\n",
      "  (0, 9)\t0.3922322702763681\n",
      "  (0, 4)\t0.19611613513818404\n",
      "  (0, 19)\t0.3922322702763681\n",
      "  (0, 7)\t0.19611613513818404\n",
      "  (0, 13)\t0.19611613513818404\n",
      "  (0, 0)\t0.19611613513818404\n",
      "  (0, 15)\t0.19611613513818404\n",
      "  (0, 8)\t0.19611613513818404\n",
      "  (0, 16)\t0.19611613513818404\n",
      "  (0, 6)\t0.19611613513818404\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_model.fit_transform([cleaned_data])\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545bbd2-d76d-4fd6-b36f-572649fa380d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
